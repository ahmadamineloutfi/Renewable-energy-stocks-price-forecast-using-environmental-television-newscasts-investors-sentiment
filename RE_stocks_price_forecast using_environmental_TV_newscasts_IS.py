# -*- coding: utf-8 -*-
"""environmental_TV_newscasts.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12myjS1E0FgBi6lulAZS5M-t-pnatt8gQ

**Author : Ahmad Amine Loutfi, PhD student, Norwegian University of Science and Tehcnology**

# **I) Alternative data: Environmental TV Newscasts- CNN, BBC News, MSNBC, and Fox News**
"""

import os
import glob

import pandas as pd
import numpy as np

import matplotlib as mpl 
import matplotlib.pyplot as plt
import seaborn as sb

from google.colab import drive
drive.mount('/content/gdrive')

!unzip "/content/gdrive/MyDrive/NLP.zip"

path = '/content/TelevisionNews'
extension = 'csv'
os.chdir(path)
csv_files = glob.glob('*.{}'.format(extension))

mapper_1=map(lambda x: pd.read_csv(x, index_col=None, header=0), csv_files)
print(type(mapper_1))

mapper_2=list(mapper_1)
print(len(mapper_2))

ad= pd.concat(mapper_2,axis=0,ignore_index=True)
print('lengh of dataset:',len(ad))
print(ad.info())

ad['Date'] = pd.to_datetime(ad['MatchDateTime']).dt.date

ad=ad.loc[:,['Date','Station','Snippet']]

ad.sort_values(by='Date', inplace=True)

ad.set_index('Date', inplace=True)

"""* Missing values"""

missing_dates_1=pd.date_range(start = '2009-07-02', end = '2020-01-21',freq="D" ).difference(ad.index)
ad.reset_index(inplace=True,drop=False)

"""## **k**

#**i) Liberals: CNN, BBC News,and  MSNBC**
* To proceed with liberals solely, please unmark the code below.
"""

#ad = ad[(ad["Station"] == 'CNN') | (ad["Station"] == 'BBCNEWS')| (ad["Station"] == 'MSNBC')]

#ad

"""#**ii) Conservatives:Fox News**
* To proceed with conservatives solely, please unmark the code below.
"""

#ad= ad[ad["Station"] == 'FOXNEWS']
#ad

"""#**Corpus Analysis**"""

ad['word_count']=ad['Snippet'].apply (lambda x: len(x.split()))
print('lengh of dataset:',len(ad))
ad.head()

ad['char_count']=ad['Snippet'].apply (lambda x: len(x))
print('lengh of dataset:',len(ad))
ad.head()

def average_words(x):
  words=x.split()
  return sum(len(word) for word in words)/len(words)

ad['average_word_lengh']=ad['Snippet'].apply (lambda x: average_words(x))
print('lengh of dataset:',len(ad))
ad.head()

!pip install nltk

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords

stop_words=stopwords.words('english')
print(stop_words)
len(stop_words)

ad['stop_word_count']=ad['Snippet'].apply (lambda x: len ([word for word in x.split() if word.lower() in stop_words]))
print('lengh of dataset:',len(ad))
ad.head()

ad['stop_word_count_rate']= ad['stop_word_count']  / ad['word_count']
print('lengh of dataset:',len(ad))
ad.head()

ad.sort_values(by='stop_word_count_rate')
print('lengh of dataset:',len(ad))
ad.head()

ad.describe()

pd.DataFrame(ad).kurtosis()

pd.DataFrame(ad).skew()

duplicate_1 = ad[ad["Snippet"].duplicated()]
 
print('lengh of duplicate rown:',len(duplicate_1))
print("Duplicate Rows :")
duplicate_1[0:10]


ad_merged = ad.drop_duplicates(subset=["Snippet","Date"], keep='last')
print('lengh of dataset:',len(ad_merged))
ad_merged.head()

ad_merged=ad_merged.groupby(by='Date').agg({'Snippet': 'sum'})
ad_merged

ad_merged['word_count_merged']=ad_merged['Snippet'].apply (lambda x: len(x.split()))
print('lengh of dataset:',len(ad_merged))
ad_merged.head()

ad_merged['char_count_merged']=ad_merged['Snippet'].apply (lambda x: len(x))
print('lengh of dataset:',len(ad_merged))
ad_merged.head()

ad_merged['average_word_lengh_merged']=ad_merged['Snippet'].apply (lambda x: average_words(x))
print('lengh of dataset:',len(ad_merged))
ad_merged.head()

ad_merged['stop_word_count_nerged']=ad_merged['Snippet'].apply (lambda x: len ([word for word in x.split() if word.lower() in stop_words]))
print('lengh of dataset:',len(ad_merged))
ad_merged.head()

ad_merged['stop_word_count_merged']=ad_merged['Snippet'].apply (lambda x: len ([word for word in x.split() if word.lower() in stop_words]))
print('lengh of dataset:',len(ad_merged))
ad_merged.head()

ad_merged['stop_word_count_rate_merged']= ad_merged['stop_word_count_merged']  / ad_merged['word_count_merged']
print('lengh of dataset:',len(ad_merged))
ad_merged.head()

ad_merged.describe().round(2)

pd.DataFrame(ad_merged).median().round(2)

pd.DataFrame(ad_merged).kurtosis()

pd.DataFrame(ad_merged).skew()

"""**---------------Pickle---------------**"""

ad.to_pickle("./corpus.pkl")

"""**---------------Un-Pickle---------------**"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np

import matplotlib as mpl 
import matplotlib.pyplot as plt
import seaborn as sb

ad = pd.read_pickle("/content/gdrive/MyDrive/corpus.pkl") 
ad

"""#**Corpus Cleaning**"""

import re
import string

def clean_text_round1(text):
    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''
    text = text.lower()
    text = re.sub('\[.*?\]', ' ', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)
    text = re.sub('\w*\d\w*', ' ', text)
    return text

round1 = lambda x: clean_text_round1(x)
ad['Snippet_1'] = ad['Snippet'].apply(round1)
print('lengh of dataset:',len(ad))

ad.head()

def clean_text_round2(text):
    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''
    text = re.sub('[‘’“”…]', ' ', text)
    text = re.sub('\n', ' ', text)
    return text

round2 = lambda x: clean_text_round2(x)
ad['Snippet_2'] = ad['Snippet_1'].apply(round2)

print('lengh of dataset:',len(ad))
ad.head()

duplicate = ad[ad["Snippet_2"].duplicated()]
 
print('lengh of duplicate:',len(duplicate))
print("Duplicate Rows :")
duplicate[0:10]

ad = ad.drop_duplicates(subset=["Snippet","Date"], keep='last')
print('lengh of dataset:',len(ad))
ad.head()

ad.set_index('Date', inplace=True)

missing_dates_2=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(ad.index)
print('lengh of dataset:',len(missing_dates_2))
print(missing_dates_2)

ad.reset_index(inplace=True,drop=False)

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words=stopwords.words('english')
print(stop_words)
len(stop_words)

ad['Snippet_4']=ad['Snippet_2'].apply (lambda x: " ".join(word for word in x.split() if word not in stop_words))
print('lengh of dataset:',len(ad))
ad.head()

import spacy

en = spacy.load('en_core_web_sm')
sw_spacy = en.Defaults.stop_words
print(sw_spacy)
print(type(sw_spacy))
print(len(sw_spacy))

ad['Snippet_5']=ad['Snippet_4'].apply (lambda x: " ".join(word for word in x.split() if word not in sw_spacy))
print('lengh of dataset:',len(ad))
ad.head()

import gensim
from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS
print(STOPWORDS)
print(len(STOPWORDS))

ad['Snippet_6']=ad['Snippet_5'].apply (lambda x: " ".join(word for word in x.split() if word not in STOPWORDS))
print('lengh of dataset:',len(ad))
ad.head()

"""**------------------Pickle------------------**"""

ad.to_pickle("./corpus_cleaned_1.pkl")

"""**------------------Un-Pickle------------------**"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np

import matplotlib as mpl 
import matplotlib.pyplot as plt
import seaborn as sb
ad = pd.read_pickle("/content/gdrive/MyDrive/corpus_cleaned_1.pkl")
ad

for each in ad['Snippet_6']:
  print(each)

! pip install autocorrect

from autocorrect import Speller
spell = Speller()

for i in range(0,92710,5):
  print(i)
  Snippet_7=ad['Snippet_6'][i:i+5].apply(lambda x: str(spell(x)))
  print(Snippet_7)
  with open('Snippet_7.txt','a') as fs:
    for each in Snippet_7:
      fs.write(each)
      fs.write('\n')

ad_Snippet_7 = pd.read_csv('/content/gdrive/MyDrive/Snippet_7.txt', header = None,names=["Snippet_7"])
ad_Snippet_7

ad=pd.concat([ad, ad_Snippet_7], axis=1)
ad

"""**------------------Pickle------------------**"""

ad.to_pickle("./corpus_cleaned_2.pkl")

"""**---------------Unpickle------------------**"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np

import matplotlib as mpl 
import matplotlib.pyplot as plt
import seaborn as sb
ad = pd.read_pickle("/content/gdrive/MyDrive/corpus_cleaned_2.pkl")
ad

nltk.download('wordnet')
nltk.download('omw-1.4')

from textblob import Word

ad['Snippet_8']=ad['Snippet_7'].apply (lambda x: " ".join(Word(word).lemmatize() for word in x.split()))
print('lengh of dataset:',len(ad))
ad.head()

few_words=ad.loc[ad['Snippet_8'].str.split().apply(len) < 4]
print('lengh of dataset:',len(few_words))
few_words

ad=ad.loc[ad['Snippet_8'].str.split().apply(len) >= 3]
print('lengh of dataset:',len(ad))
ad

ad.set_index('Date', inplace=True)

missing_dates_3=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(ad.index)
print('lengh of dataset:',len(missing_dates_3))
print(missing_dates_3)

ad.reset_index(inplace=True,drop=False)

ad['Snippet_9']=ad.Snippet_8.str.replace(r'\b(\w{1,2})\b', ' ')
print('lengh of dataset:',len(ad))
ad.head()

list = " ".join(ad['Snippet_9']).split()
elm_count = list.count('cha')
print('The count of the word  is ', elm_count)
print('lengh of dataset:',len(ad))

one_string = ' '.join(ad['Snippet_9'].tolist())
one_string

counts = {}
for word in one_string.lower().split():
  if word in counts:
    counts[word]+=1
  else:
      counts[word]=1
stop_words_selected= [word for word in counts if counts[word]==1]
other_stop_words=set(stop_words_selected)
print(type (other_stop_words))
print(len(other_stop_words))
other_stop_words

ad['Snippet_10']=ad['Snippet_9'].apply (lambda x: " ".join(word for word in x.split() if word not in other_stop_words))
print('lengh of dataset:',len(ad))
ad.head()

select_stop_words=['cha','obama','donald','trump','white house','government','president']
len(select_stop_words)

ad['Snippet_11']=ad['Snippet_10'].apply (lambda x: " ".join(word for word in x.split() if word not in select_stop_words))
print('lengh of dataset:',len(ad))
ad.head()

"""#**Cleanned Corpus Analysis**"""

ad['Clean_Snippet']=ad['Snippet_11']
ad

ad.set_index('Date', inplace=True)
ad.head()

ad.sort_values(by='Date', inplace=True)
print('lengh of dataset:',len(ad))
ad.head()

def average_words(x):
  words=x.split()
  return sum(len(word) for word in words)/len(words)
  
ad['word_count_new']=ad['Clean_Snippet'].apply (lambda x: len(x.split()))
ad['char_count_new']=ad['Clean_Snippet'].apply (lambda x: len(x))
ad['average_word_lengh_new']=ad['Clean_Snippet'].apply (lambda x: average_words(x))
ad.head()

"""**------------------Pickle----------------------------**"""

ad.to_pickle("./corpus_cleaned_3.pkl")

"""**------------------Un-Pickle----------------------------**"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np

import matplotlib as mpl 
import matplotlib.pyplot as plt
import seaborn as sb
ad = pd.read_pickle("/content/gdrive/MyDrive/corpus_cleaned_3.pkl")
ad

! pip install wordcloud

from wordcloud import WordCloud, STOPWORDS

text = ' '.join(ad['Clean_Snippet'].tolist())
text

wordcloud = WordCloud().generate(text)

mpl.rcParams['figure.figsize']=(10,10)

wordcloud = WordCloud(max_font_size=50, max_words=500, background_color="white").generate(text)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

wordcloud = WordCloud(max_font_size=50, max_words=500, background_color="black").generate(text)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

ad=ad.loc[:,['Station','Clean_Snippet']]
print('type:',type(ad))
ad

pd.Series(" ".join(ad['Clean_Snippet']).split()).value_counts()

missing_dates_4=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(ad.index)
print('lengh of dataset:',len(missing_dates_4))
print(missing_dates_4)

"""#**Sentiment Analysis**"""

ad_merged=ad.groupby(by='Date').agg({'Clean_Snippet': 'sum'})
ad_merged

from textblob import TextBlob

pol = lambda x: TextBlob(x).sentiment.polarity
sub = lambda x: TextBlob(x).sentiment.subjectivity

ad_merged['Polarity_merged'] = ad_merged['Clean_Snippet'].apply(pol)
ad_merged['Subjectivity_merged'] = ad_merged['Clean_Snippet'].apply(sub)

ad_merged

ad_new=pd.concat([blob_median,blob_mean,vader_compound_median,vader_compound_mean,ad_merged], axis=1,join='inner',verify_integrity=False, ignore_index=False)
pd.set_option("display.max_rows",None,"display.max_columns",None)
pd.reset_option("display.max_rows","display.max_columns")
ad_new

ad_new.describe()

"""# **Missing Values**"""

missing_dates_5=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(ad_new.index)
print('lengh of dataset:',len(missing_dates_5))
missing_dates_5

idx_1 = pd.date_range(start = '2009-07-02', end='2020-01-21')


ad_new.index = pd.DatetimeIndex(ad_new.index)

ad_new = ad_new.reindex(idx_1, fill_value=np.NaN)

print('lengh of dataset:',len(ad_new))
ad_new

ad_new=ad_new.interpolate(method="spline",order=3)
ad_new

startdate = pd.to_datetime("2009-07-02").date()
enddate = pd.to_datetime("2020-01-21").date()
ad_new=ad_new.loc[startdate:enddate]
ad_new

"""**-----------------------Pickle---------------**"""

ad_new.to_pickle("./sentiment.pkl")

"""**-----------------------UnPickle---------------**"""

from google.colab import drive
drive.mount('/content/gdrive')

#Pandas & numpy
import pandas as pd
import numpy as np

#Plotting
import matplotlib as mpl 
import matplotlib.pyplot as plt
import seaborn as sb
ad_new = pd.read_pickle("/content/gdrive/MyDrive/sentiment.pkl")
ad_new

mpl.rcParams['figure.figsize']=(12,6)
mpl.rcParams['axes.grid']=True

#p_1=sb.lineplot(data=ad, x="Date", y="Subjectivity_median")

p_1=sb.lineplot(data = ad_new['Subjectivity_merged'])
sb.set_style("whitegrid")
p_1.set( xlabel = "Date", ylabel = "Subjectivity")
plt.show()

mpl.rcParams['figure.figsize']=(12,6)
mpl.rcParams['axes.grid']=True

#p_1=sb.lineplot(data=ad, x="Date", y="Subjectivity_median")

p_1=sb.lineplot(data = ad_new['Polarity_merged'])
sb.set_style("whitegrid")
p_1.set( xlabel = "Date", ylabel = "Polarity")
plt.show()

ad_new.describe().round(2)

ad_new.median().round(2)

"""#**II) Conventional Data: Renewable nergy stocks price, S&P500, VIX index, US Dollar/USDX index-cash**

"""

!pip install yfinance

import yfinance as yf

#stock 1
dy_1_1 = yf.download('TSLA', start = '2009-07-02', end='2020-01-22').reset_index()
#stock 2
dy_1_2 = yf.download('AMSC', start = '2009-07-02', end='2020-01-21').reset_index()
#stock 3
dy_1_3 = yf.download('APD', start = '2009-07-02', end='2020-01-22').reset_index()
#stock 4
dy_1_4 = yf.download('BLDP', start = '2009-07-02', end='2020-01-22').reset_index()
#stock 5
dy_1_5 = yf.download('FCEL', start = '2009-07-02', end='2020-01-22').reset_index()
#stock 6
dy_1_6 = yf.download('ITRI', start = '2009-07-02', end='2020-01-22').reset_index()
#stock 7
dy_1_7 = yf.download('ORA', start = '2009-07-02', end='2020-01-22').reset_index()
#stock 8
dy_1_8 = yf.download('PWR', start = '2009-07-02', end='2020-01-21').reset_index()
#stock 9
dy_1_9 = yf.download('SQM', start = '2009-07-02', end='2020-01-22').reset_index()
#stock 10
dy_1_10 = yf.download('OLED', start = '2009-07-02', end='2020-01-22').reset_index()
#stock 11
dy_1_11 = yf.download('SPWR', start = '2009-07-02', end='2020-01-22').reset_index()
#stock 12
dy_1_12 = yf.download('^GSPC', start = '2009-07-02', end='2020-01-22').reset_index()
#stock 13
dy_1_13 = yf.download('^VIX', start = '2009-07-02', end='2020-01-22').reset_index()
#stock 14
dy_1_14 = yf.download('DX-Y.NYB', start = '2009-07-02', end='2020-01-22').reset_index()


print('lengh stock 1:',len (dy_1_1))
print('lengh stock 2:',len (dy_1_2))
print('lengh stock 3:',len (dy_1_3))
print('lengh stock 4:',len (dy_1_4))
print('lengh stock 5:',len (dy_1_5))
print('lengh stock 6:',len (dy_1_6))
print('lengh stock 7:',len (dy_1_7))
print('lengh stock 8:',len (dy_1_8))
print('lengh stock 9:',len (dy_1_9))
print('lengh stock 10:',len (dy_1_10))
print('lengh stock 11:',len (dy_1_11))
print('lengh stock 12:',len (dy_1_12))
print('lengh stock 13:',len (dy_1_13))
print('lengh stock 14:',len (dy_1_14))

dy_1_1['Date'] = pd.to_datetime(dy_1_1['Date']).dt.date
dy_1_1.set_index('Date', inplace=True)

dy_1_2['Date'] = pd.to_datetime(dy_1_2['Date']).dt.date
dy_1_2.set_index('Date', inplace=True)

dy_1_3['Date'] = pd.to_datetime(dy_1_3['Date']).dt.date
dy_1_3.set_index('Date', inplace=True)

dy_1_4['Date'] = pd.to_datetime(dy_1_4['Date']).dt.date
dy_1_4.set_index('Date', inplace=True)

dy_1_5['Date'] = pd.to_datetime(dy_1_5['Date']).dt.date
dy_1_5.set_index('Date', inplace=True)

dy_1_6['Date'] = pd.to_datetime(dy_1_6['Date']).dt.date
dy_1_6.set_index('Date', inplace=True)

dy_1_7['Date'] = pd.to_datetime(dy_1_7['Date']).dt.date
dy_1_7.set_index('Date', inplace=True)

dy_1_8['Date'] = pd.to_datetime(dy_1_8['Date']).dt.date
dy_1_8.set_index('Date', inplace=True)

dy_1_9['Date'] = pd.to_datetime(dy_1_9['Date']).dt.date
dy_1_9.set_index('Date', inplace=True)

dy_1_10['Date'] = pd.to_datetime(dy_1_10['Date']).dt.date
dy_1_10.set_index('Date', inplace=True)

dy_1_11['Date'] = pd.to_datetime(dy_1_11['Date']).dt.date
dy_1_11.set_index('Date', inplace=True)

dy_1_12['Date'] = pd.to_datetime(dy_1_12['Date']).dt.date
dy_1_12.set_index('Date', inplace=True)

dy_1_13['Date'] = pd.to_datetime(dy_1_13['Date']).dt.date
dy_1_13.set_index('Date', inplace=True)

dy_1_14['Date'] = pd.to_datetime(dy_1_14['Date']).dt.date
dy_1_14.set_index('Date', inplace=True)


print('lengh of dataset 1:',len(dy_1_1))
print('lengh of dataset 2:',len(dy_1_2))
print('lengh of dataset 3:',len(dy_1_3))
print('lengh of dataset 4:',len(dy_1_4))
print('lengh of dataset 5:',len(dy_1_5))
print('lengh of dataset 6:',len(dy_1_6))
print('lengh of dataset 7:',len(dy_1_7))
print('lengh of dataset 8:',len(dy_1_8))
print('lengh of dataset 9:',len(dy_1_9))
print('lengh of dataset 10:',len(dy_1_10))
print('lengh of dataset 11:',len(dy_1_11))
print('lengh of dataset 12:',len(dy_1_12))
print('lengh of dataset 13:',len(dy_1_13))
print('lengh of dataset 14:',len(dy_1_14))

# Stock 1
missing_dates_1_1=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy_1_1.index)
print('lengh of dataset:',len(missing_dates_1_1))
# Stock 2
missing_dates_1_2=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy_1_2.index)
print('lengh of dataset:',len(missing_dates_1_2))
# Stock 3
missing_dates_1_3=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy_1_3.index)
print('lengh of dataset:',len(missing_dates_1_3))
# Stock 4
missing_dates_1_4=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy_1_4.index)
print('lengh of dataset:',len(missing_dates_1_4))
# Stock 5
missing_dates_1_5=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy_1_5.index)
print('lengh of dataset:',len(missing_dates_1_5))
# Stock 6
missing_dates_1_6=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy_1_6.index)
print('lengh of dataset:',len(missing_dates_1_6))
# Stock 7
missing_dates_1_7=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy_1_7.index)
print('lengh of dataset:',len(missing_dates_1_7))
# Stock 8
missing_dates_1_8=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy_1_8.index)
print('lengh of dataset:',len(missing_dates_1_8))
# Stock 9
missing_dates_1_9=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy_1_9.index)
print('lengh of dataset:',len(missing_dates_1_9))
# Stock 10
missing_dates_1_10=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy_1_10.index)
print('lengh of dataset:',len(missing_dates_1_10))
# Stock 11
missing_dates_1_11=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy_1_11.index)
print('lengh of dataset:',len(missing_dates_1_11))
# Stock 12
missing_dates_1_12=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy_1_12.index)
print('lengh of dataset:',len(missing_dates_1_12))
# Stock 13
missing_dates_1_13=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy_1_13.index)
print('lengh of dataset:',len(missing_dates_1_13))
# Stock 14
missing_dates_1_14=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy_1_14.index)
print('lengh of dataset:',len(missing_dates_1_14))

dy_1_1['Return_1'] = (dy_1_1['Adj Close']/ dy_1_1['Adj Close'].shift(1)) -1
dy_1_2['Return_2'] = (dy_1_2['Adj Close']/ dy_1_2['Adj Close'].shift(1)) -1
dy_1_3['Return_3'] = (dy_1_3['Adj Close']/ dy_1_3['Adj Close'].shift(1)) -1
dy_1_4['Return_4'] = (dy_1_4['Adj Close']/ dy_1_4['Adj Close'].shift(1)) -1
dy_1_5['Return_5'] = (dy_1_5['Adj Close']/ dy_1_5['Adj Close'].shift(1)) -1
dy_1_6['Return_6'] = (dy_1_6['Adj Close']/ dy_1_6['Adj Close'].shift(1)) -1
dy_1_7['Return_7'] = (dy_1_7['Adj Close']/ dy_1_7['Adj Close'].shift(1)) -1
dy_1_8['Return_8'] = (dy_1_8['Adj Close']/ dy_1_8['Adj Close'].shift(1)) -1
dy_1_9['Return_9'] = (dy_1_9['Adj Close']/ dy_1_9['Adj Close'].shift(1)) -1
dy_1_10['Return_10'] = (dy_1_10['Adj Close']/ dy_1_10['Adj Close'].shift(1)) -1
dy_1_11['Return_11'] = (dy_1_11['Adj Close']/ dy_1_11['Adj Close'].shift(1)) -1
dy_1_12['Return_12'] = (dy_1_12['Adj Close']/ dy_1_12['Adj Close'].shift(1)) -1
dy_1_13['Return_13'] = (dy_1_13['Adj Close']/ dy_1_13['Adj Close'].shift(1)) -1
dy_1_14['Return_14'] = (dy_1_14['Adj Close']/ dy_1_14['Adj Close'].shift(1)) -1

data = [dy_1_1['Adj Close'],dy_1_2['Adj Close'],dy_1_3['Adj Close'],dy_1_4['Adj Close'],dy_1_5['Adj Close'],
        dy_1_6['Adj Close'],dy_1_7['Adj Close'], dy_1_8['Adj Close'],dy_1_9['Adj Close'],dy_1_10['Adj Close'],
        dy_1_11['Adj Close'],dy_1_12['Adj Close'],dy_1_13['Adj Close'],dy_1_14['Adj Close'],
       
        dy_1_1['Return_1'],dy_1_2['Return_2'],dy_1_3['Return_3'],dy_1_4['Return_4'],dy_1_5['Return_5'],
        dy_1_6['Return_6'],dy_1_7['Return_7'], dy_1_8['Return_8'],dy_1_9['Return_9'],dy_1_10['Return_10'],
        dy_1_11['Return_11'],dy_1_12['Return_12'],dy_1_13['Return_13'],dy_1_14['Return_14']]


headers = ["American Superconductor","Air Products & Chem","Ballard Power Systems","FuelCell Energy","Itron","Ormat Technologies","Quanta Services","Sociedad Quimica y Minera","Universal Display","Sunpower",
           "S&P 500","VIX","USD",
           
           'Return_1','Return_2','Return_3','Return_4','Return_5',
           'Return_6','Return_7','Return_8','Return_9','Return_10',
           'Return_11','Return_12','Return_13','Return_14']

data_new = pd.concat(data, axis=1, keys=headers)
print('lengh of dataset:',len(data))
data_new

data_new.describe().round(2)

data_new.median().round(2)

dy_1_1.describe().round(2)

dy_1_1.median().round(2)

# stock 1
idx_1_1 = pd.date_range(start = '2009-07-02', end='2020-01-21')
dy_1_1.index = pd.DatetimeIndex(dy_1_1.index)
dy_1_1_new = dy_1_1.reindex(idx_1_1, fill_value=np.NaN)
print('lengh of stock 1:',len(dy_1_1_new))
# stock 2
idx_1_2 = pd.date_range(start = '2009-07-02', end='2020-01-21')
dy_1_2.index = pd.DatetimeIndex(dy_1_2.index)
dy_1_2_new = dy_1_2.reindex(idx_1_2, fill_value=np.NaN)
print('lengh of stock 2:',len(dy_1_2_new))
# stock 3
idx_1_3 = pd.date_range(start = '2009-07-02', end='2020-01-21')
dy_1_3.index = pd.DatetimeIndex(dy_1_3.index)
dy_1_3_new = dy_1_3.reindex(idx_1_3, fill_value=np.NaN)
print('lengh of stock 3:',len(dy_1_3_new))
# stock 4
idx_1_4 = pd.date_range(start = '2009-07-02', end='2020-01-21')
dy_1_4.index = pd.DatetimeIndex(dy_1_4.index)
dy_1_4_new = dy_1_4.reindex(idx_1_4, fill_value=np.NaN)
print('lengh of stock 4:',len(dy_1_4_new))
# stock 5
idx_1_5 = pd.date_range(start = '2009-07-02', end='2020-01-21')
dy_1_5.index = pd.DatetimeIndex(dy_1_5.index)
dy_1_5_new = dy_1_5.reindex(idx_1_5, fill_value=np.NaN)
print('lengh of stock 5:',len(dy_1_5_new))
# stock 6
idx_1_6 = pd.date_range(start = '2009-07-02', end='2020-01-21')
dy_1_6.index = pd.DatetimeIndex(dy_1_6.index)
dy_1_6_new = dy_1_6.reindex(idx_1_6, fill_value=np.NaN)
print('lengh of stock 6:',len(dy_1_6_new))
# stock 7
idx_1_7 = pd.date_range(start = '2009-07-02', end='2020-01-21')
dy_1_7.index = pd.DatetimeIndex(dy_1_7.index)
dy_1_7_new = dy_1_7.reindex(idx_1_7, fill_value=np.NaN)
print('lengh of stock 7:',len(dy_1_7_new))
# stock 8
idx_1_8 = pd.date_range(start = '2009-07-02', end='2020-01-21')
dy_1_8.index = pd.DatetimeIndex(dy_1_8.index)
dy_1_8_new = dy_1_8.reindex(idx_1_8, fill_value=np.NaN)
print('lengh of stock 8:',len(dy_1_8_new))
# stock 9
idx_1_9 = pd.date_range(start = '2009-07-02', end='2020-01-21')
dy_1_9.index = pd.DatetimeIndex(dy_1_9.index)
dy_1_9_new = dy_1_9.reindex(idx_1_9, fill_value=np.NaN)
print('lengh of stock 9:',len(dy_1_9_new))
# stock 10
idx_1_10 = pd.date_range(start = '2009-07-02', end='2020-01-21')
dy_1_10.index = pd.DatetimeIndex(dy_1_10.index)
dy_1_10_new = dy_1_10.reindex(idx_1_10, fill_value=np.NaN)
print('lengh of stock 10:',len(dy_1_10_new))
# stock 11
idx_1_11 = pd.date_range(start = '2009-07-02', end='2020-01-21')
dy_1_11.index = pd.DatetimeIndex(dy_1_11.index)
dy_1_11_new = dy_1_11.reindex(idx_1_11, fill_value=np.NaN)
print('lengh of stock 11:',len(dy_1_11_new))
# stock 12
idx_1_12 = pd.date_range(start = '2009-07-02', end='2020-01-21')
dy_1_12.index = pd.DatetimeIndex(dy_1_10.index)
dy_1_12_new = dy_1_12.reindex(idx_1_12, fill_value=np.NaN)
print('lengh of stock 12:',len(dy_1_12_new))
# stock 13
idx_1_13 = pd.date_range(start = '2009-07-02', end='2020-01-21')
dy_1_13.index = pd.DatetimeIndex(dy_1_13.index)
dy_1_13_new = dy_1_13.reindex(idx_1_13, fill_value=np.NaN)
print('lengh of stock 13',len(dy_1_13_new))
# stock 14
idx_1_14 = pd.date_range(start = '2009-07-02', end='2020-01-21')
dy_1_14.index = pd.DatetimeIndex(dy_1_14.index)
dy_1_14_new = dy_1_14.reindex(idx_1_14, fill_value=np.NaN)
print('lengh of stock 14:',len(dy_1_14_new))

dy_1_1_final=dy_1_1_new.interpolate(method="spline",order=3)
dy_1_2_final=dy_1_2_new.interpolate(method="spline",order=3)
dy_1_3_final=dy_1_3_new.interpolate(method="spline",order=3)
dy_1_4_final=dy_1_4_new.interpolate(method="spline",order=3)
dy_1_5_final=dy_1_5_new.interpolate(method="spline",order=3)
dy_1_6_final=dy_1_6_new.interpolate(method="spline",order=3)
dy_1_7_final=dy_1_7_new.interpolate(method="spline",order=3)
dy_1_8_final=dy_1_8_new.interpolate(method="spline",order=3)
dy_1_9_final=dy_1_9_new.interpolate(method="spline",order=3)
dy_1_10_final=dy_1_10_new.interpolate(method="spline",order=3)

dy_1_11_final=dy_1_11_new.interpolate(method="spline",order=3)
dy_1_12_final=dy_1_12_new.interpolate(method="spline",order=3)
dy_1_13_final=dy_1_13_new.interpolate(method="spline",order=3)
dy_1_14_final=dy_1_14_new.interpolate(method="spline",order=3)

dy_1_1_final.reset_index(inplace=True,drop=False)
dy_1_2_final.reset_index(inplace=True,drop=False)
dy_1_3_final.reset_index(inplace=True,drop=False)
dy_1_4_final.reset_index(inplace=True,drop=False)
dy_1_5_final.reset_index(inplace=True,drop=False)
dy_1_6_final.reset_index(inplace=True,drop=False)
dy_1_7_final.reset_index(inplace=True,drop=False)
dy_1_8_final.reset_index(inplace=True,drop=False)
dy_1_9_final.reset_index(inplace=True,drop=False)
dy_1_10_final.reset_index(inplace=True,drop=False)

dy_1_11_final.reset_index(inplace=True,drop=False)
dy_1_12_final.reset_index(inplace=True,drop=False)
dy_1_13_final.reset_index(inplace=True,drop=False)
dy_1_14_final.reset_index(inplace=True,drop=False)

dy_1_1_final.rename(columns={'index': 'Date'}, inplace=True)

dy_1 = [dy_1_1_final['Date'],
        dy_1_1_final['Adj Close'], dy_1_2_final['Adj Close'],dy_1_3_final['Adj Close'],dy_1_4_final['Adj Close'],dy_1_5_final['Adj Close'],
        dy_1_6_final['Adj Close'],dy_1_7_final['Adj Close'], dy_1_8_final['Adj Close'],dy_1_9_final['Adj Close'],dy_1_10_final['Adj Close'],
        dy_1_11_final['Adj Close'],dy_1_12_final['Adj Close'],dy_1_13_final['Adj Close'],dy_1_14_final['Adj Close']
        ]

headers = ["Date",
           "Tesla", "American Superconductor","Air Products & Chem","Ballard Power Systems","FuelCell Energy","Itron","Ormat Technologies","Quanta Services","Sociedad Quimica y Minera","Universal Display","Sunpower",
           "S&P 500","VIX","USD",
           ]

dy_2 = pd.concat(dy_1, axis=1, keys=headers)
print('lengh of dataset:',len(dy_2))
dy_2

dy=dy_2.set_index('Date')
print('lengh of dataset:',len(dy))
dy

missing_dates_3=pd.date_range(start = '2009-07-02', end = '2020-01-21', freq="D" ).difference(dy.index)
print('lengh of dataset:',len(missing_dates_3))
missing_dates_3

"""**---------------Pickle---------------------**"""

dy.to_pickle("./yahoofinance.pkl")

"""**--------------------UN-Pickle------------------------**"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np

import matplotlib as mpl 
import matplotlib.pyplot as plt
import seaborn as sb
dy= pd.read_pickle("/content/gdrive/MyDrive/yahoofinance.pkl")
dy

"""#**III) Full Dataset: alternative data and convenional data**"""

df_1=pd.concat([ad_new, dy.reindex(ad_new.index)], axis=1)
df_2=df_1[["Tesla", "American Superconductor","Air Products & Chem","Ballard Power Systems","FuelCell Energy","Itron","Ormat Technologies","Quanta Services","Sociedad Quimica y Minera","Universal Display","Sunpower",
           
           "S&P 500","VIX","USD",
           
           'Polarity_median','Subjectivity_median','Polarity_mean','Subjectivity_mean',	'Vader_compound_median',	'Vader_compound_mean',	'Polarity_merged','Subjectivity_merged','Vader_compound_merged',
           ]]
df_1

startdate = pd.to_datetime("2009-07-02").date()
enddate = pd.to_datetime("2020-01-21").date()
df=df_1.loc[startdate:enddate]
df

df['Tesla']=df['Tesla'].ffill(axis ='rows').bfill(axis ='rows')  
df

startdate = pd.to_datetime("2009-07-02").date()
enddate = pd.to_datetime("2020-01-21").date()
df=df.loc[startdate:enddate]
df

"""**---------------Pickle---------------------**"""

df.to_pickle("./full_dataset.pkl")

"""**--------------------UN-Pickle------------------------**"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np

import matplotlib as mpl 
import matplotlib.pyplot as plt
import seaborn as sb
df= pd.read_pickle("/content/gdrive/MyDrive/full_dataset.pkl")
df

"""#**IV) Correlation analysis**"""

df_corr=df[["American Superconductor","Air Products & Chem","Ballard Power Systems","FuelCell Energy","Itron","Tesla", "Ormat Technologies","Quanta Services","Sociedad Quimica y Minera","Universal Display","Sunpower",
            
            "S&P 500","VIX","USD",
            
            'Polarity_merged','Subjectivity_merged',
            ]]
df_corr=df_corr.rename(columns={"Polarity_merged": "Polarity","Subjectivity_merged": "Subjectivity"}, errors="raise")
df_corr.info()

startdate = pd.to_datetime("2009-07-02").date()
enddate = pd.to_datetime("2020-01-21").date()
df_corr=df_corr.loc[startdate:enddate]
df_corr

df_corr['Tesla']=df_corr['Tesla'].ffill(axis ='rows').bfill(axis ='rows')  
df_corr

mpl.rcParams['figure.figsize']=(15,15)
spearmancorr = round(df_corr.corr(method='spearman'),2)
spearmancorr
sb.heatmap(spearmancorr, 
            xticklabels=spearmancorr.columns,
            yticklabels=spearmancorr.columns,
            cmap='Blues_r',
            annot=True,
            linewidth=0.5)

import scipy

print(scipy.__version__)

pol_1=pd.DataFrame(df_corr['Polarity']).to_numpy()
sub_1=pd.DataFrame(df_corr['Subjectivity']).to_numpy()

adj_close_1=pd.DataFrame(df_corr['American Superconductor']).to_numpy()
adj_close_2=pd.DataFrame(df_corr['Air Products & Chem']).to_numpy()
adj_close_3=pd.DataFrame(df_corr['Ballard Power Systems']).to_numpy()
adj_close_4=pd.DataFrame(df_corr['FuelCell Energy']).to_numpy()
adj_close_5=pd.DataFrame(df_corr['Itron']).to_numpy()
adj_close_6=pd.DataFrame(df_corr['Tesla']).to_numpy()
adj_close_7=pd.DataFrame(df_corr['Ormat Technologies']).to_numpy()
adj_close_8=pd.DataFrame(df_corr['Quanta Services']).to_numpy()
adj_close_9=pd.DataFrame(df_corr['Sociedad Quimica y Minera']).to_numpy()
adj_close_10=pd.DataFrame(df_corr['Universal Display']).to_numpy()
adj_close_11=pd.DataFrame(df_corr['Sunpower']).to_numpy()
adj_close_12=pd.DataFrame(df_corr['S&P 500']).to_numpy()
adj_close_13=pd.DataFrame(df_corr['VIX']).to_numpy()
adj_close_14=pd.DataFrame(df_corr['USD']).to_numpy()


print(type(pol_1))
print(type (sub_1))
print(type(adj_close_1))
print(type(adj_close_2))
print(type(adj_close_3))
print(type(adj_close_4))
print(type(adj_close_5))
print(type(adj_close_6))
print(type(adj_close_7))
print(type(adj_close_8))
print(type(adj_close_9))
print(type(adj_close_10))
print(type(adj_close_11))
print(type(adj_close_12))
print(type(adj_close_13))
print(type(adj_close_14))

pol= np.reshape(pol_1, [3856])
sub= np.reshape(sub_1, [3856])
adj_close_1= np.reshape(adj_close_1, [3856])
adj_close_2= np.reshape(adj_close_2, [3856])
adj_close_3= np.reshape(adj_close_3, [3856])
adj_close_4= np.reshape(adj_close_4, [3856])
adj_close_5= np.reshape(adj_close_5, [3856])
adj_close_6= np.reshape(adj_close_6, [3856])
adj_close_7= np.reshape(adj_close_7, [3856])
adj_close_8= np.reshape(adj_close_8, [3856])
adj_close_9= np.reshape(adj_close_9, [3856])
adj_close_10= np.reshape(adj_close_10, [3856])
adj_close_11= np.reshape(adj_close_11, [3856])
adj_close_12= np.reshape(adj_close_12, [3856])
adj_close_13= np.reshape(adj_close_13, [3856])
adj_close_14= np.reshape(adj_close_14, [3856])

from scipy.stats.stats import pearsonr

print('<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>')
print('Pearson_Polarity')  
print('<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>')


print( pearsonr(adj_close_1, pol))
r_29 = round(pearsonr(adj_close_1, pol)[0], 2)
print(r_29)
p_29 = round(pearsonr(adj_close_1, pol)[1], 2)
print(p_29)

print('-------------------------')
print( pearsonr(adj_close_2, pol))
r_30 = round(pearsonr(adj_close_2, pol)[0], 2)
print(r_30)
p_30 = round(pearsonr(adj_close_2, pol)[1], 2)
print(p_30)

print('-------------------------')
print( pearsonr(adj_close_3, pol))
r_31 = round(pearsonr(adj_close_3, pol)[0], 2)
print(r_31)
p_31 = round(pearsonr(adj_close_3, pol)[1], 2)
print(p_31)

print('-------------------------')
print( pearsonr(adj_close_4, pol))
r_32 = round(pearsonr(adj_close_4, pol)[0], 2)
print(r_32)
p_32 = round(pearsonr(adj_close_4, pol)[1], 2)
print(p_32)

print('-------------------------')
print( pearsonr(adj_close_5, pol))
r_33 = round(pearsonr(adj_close_5, pol)[0], 2)
print(r_33)
p_33 = round(pearsonr(adj_close_5, pol)[1], 2)
print(p_33)

print('-------------------------')
print( pearsonr(adj_close_6, pol))
r_34 = round(pearsonr(adj_close_6, pol)[0], 2)
print(r_34)
p_34 = round(pearsonr(adj_close_6, pol)[1], 2)
print(p_34)

print('-------------------------')
print( pearsonr(adj_close_7, pol))
r_35 = round(pearsonr(adj_close_7, pol)[0], 2)
print(r_35)
p_35 = round(pearsonr(adj_close_7, pol)[1], 2)
print(p_35)

print('-------------------------')
print( pearsonr(adj_close_8, pol))
r_36 = round(pearsonr(adj_close_8, pol)[0], 2)
print(r_36)
p_36 = round(pearsonr(adj_close_8, pol)[1], 2)
print(p_36)

print('-------------------------')
print( pearsonr(adj_close_9, pol))
r_37 = round(pearsonr(adj_close_9, pol)[0], 2)
print(r_37)
p_37 = round(pearsonr(adj_close_9, pol)[1], 2)
print(p_37)

print('-------------------------')
print( pearsonr(adj_close_10, pol))
r_38 = round(pearsonr(adj_close_10, pol)[0], 2)
print(r_38)
p_38 = round(pearsonr(adj_close_10, pol)[1], 2)
print(p_38)

print('-------------------------')
print( pearsonr(adj_close_11, pol))
r_39 = round(pearsonr(adj_close_11, pol)[0], 2)
print(r_39)
p_39 = round(pearsonr(adj_close_11, pol)[1], 2)
print(p_39)

print('-------------------------')
print( pearsonr(adj_close_12, pol))
r_40 = round(pearsonr(adj_close_12, pol)[0], 2)
print(r_40)
p_40 = round(pearsonr(adj_close_12, pol)[1], 2)
print(p_40)

print('-------------------------')
print( pearsonr(adj_close_13, pol))
r_41 = round(pearsonr(adj_close_13, pol)[0], 2)
print(r_41)
p_41 = round(pearsonr(adj_close_13, pol)[1], 2)
print(p_41)

print('-------------------------')
print( pearsonr(adj_close_14, pol))
r_42 = round(pearsonr(adj_close_14, pol)[0], 2)
print(r_42)
p_42 = round(pearsonr(adj_close_14, pol)[1], 2)
print(p_42)


print('<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>')
print('Pearson_Subjectivity')  
print('<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>')


print( pearsonr(adj_close_1, sub))
r_43 = round(pearsonr(adj_close_1, sub)[0], 2)
print(r_43)
p_43 = round(pearsonr(adj_close_1, sub)[1], 2)
print('p-value',p_43)

print('-------------------------')
print( pearsonr(adj_close_2, sub))
r_44 = round(pearsonr(adj_close_2, sub)[0], 2)
print(r_44)
p_44 = round(pearsonr(adj_close_2, sub)[1], 2)
print('p-value',p_44)

print('-------------------------')
print( pearsonr(adj_close_3, sub))
r_45 = round(pearsonr(adj_close_3, sub)[0], 2)
print(r_45)
p_45 = round(pearsonr(adj_close_3, sub)[1], 2)
print('p-value',p_45)

print('-------------------------')
print( pearsonr(adj_close_4, sub))
r_46 = round(pearsonr(adj_close_4, sub)[0], 2)
print(r_46)
p_46 = round(pearsonr(adj_close_4, sub)[1], 2)
print('p-value',p_46)

print('-------------------------')
print( pearsonr(adj_close_5, sub))
r_47 = round(pearsonr(adj_close_5, sub)[0], 2)
print(r_47)
p_47 = round(pearsonr(adj_close_5, sub)[1], 2)
print('p-value',p_47)

print('-------------------------')
print( pearsonr(adj_close_6, sub))
r_48 = round(pearsonr(adj_close_6, sub)[0], 2)
print(r_48)
p_48 = round(pearsonr(adj_close_6, sub)[1], 2)
print('p-value',p_48)

print('-------------------------')
print( pearsonr(adj_close_7, sub))
r_49 = round(pearsonr(adj_close_7, sub)[0], 2)
print(r_49)
p_49 = round(pearsonr(adj_close_7, sub)[1], 2)
print('p-value',p_49)

print('-------------------------')
print( pearsonr(adj_close_8, sub))
r_50 = round(pearsonr(adj_close_8, sub)[0], 2)
print(r_50)
p_50 = round(pearsonr(adj_close_8, sub)[1], 2)
print('p-value',p_50)

print('-------------------------')
print( pearsonr(adj_close_9, sub))
r_51 = round(pearsonr(adj_close_9, sub)[0], 2)
print(r_51)
p_51 = round(pearsonr(adj_close_9, sub)[1], 2)
print('p-value',p_51)

print('-------------------------')
print( pearsonr(adj_close_10, sub))
r_52 = round(pearsonr(adj_close_10, sub)[0], 2)
print(r_52)
p_52 = round(pearsonr(adj_close_10, sub)[1], 2)
print('p-value',p_52)

print('-------------------------')
print( pearsonr(adj_close_11, sub))
r_53 = round(pearsonr(adj_close_11, sub)[0], 2)
print(r_53)
p_53 = round(pearsonr(adj_close_11, sub)[1], 2)
print('p-value',p_53)

print('-------------------------')
print( pearsonr(adj_close_12, sub))
r_54 = round(pearsonr(adj_close_12, sub)[0], 2)
print(r_54)
p_54 = round(pearsonr(adj_close_12, sub)[1], 2)
print('p-value',p_54)

print('-------------------------')
print( pearsonr(adj_close_13, sub))
r_55 = round(pearsonr(adj_close_13, sub)[0], 2)
print(r_55)
p_55 = round(pearsonr(adj_close_13, sub)[1], 2)
print('p-value',p_55)

print('-------------------------')
print( pearsonr(adj_close_14, sub))
r_56 = round(pearsonr(adj_close_14, sub)[0], 2)
print(r_56)
p_56 = round(pearsonr(adj_close_14, sub)[1], 2)
print('p-value',p_56)

"""# **V) LSTM:**
* Please for each firm stock price:update the name of the firm, model lookback, and nodes to avoid fitting and underfitting.*
For tesla please use: df_tesla instead of df
"""

#Random Forest
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression

#R square
from sklearn.metrics import r2_score

#Data transformation
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.preprocessing import MinMaxScaler , StandardScaler

#Time series Generator
from keras.preprocessing.sequence import TimeseriesGenerator

#ML_Modeling
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow as tf

startdate = pd.to_datetime("2009-07-02").date()
enddate = pd.to_datetime("2020-01-21").date()
df=df.loc[startdate:enddate]
df

startdate = pd.to_datetime("2009-07-02").date()
enddate = pd.to_datetime("2020-01-21").date()
df_tesla=df.loc[startdate:enddate]
df_tesla

"""**1) Augmented model**
* N:B: Please for each firm stock price:update the name of the firm,  model lookback, and nodes to avoid fitting and underfitting. 
* For tesla please use: df_tesla instead of df
"""

df_input_1=df[['Subjectivity','S&P 500',	'VIX'	,'USD','Universal Display']]

print('lengh of columns:',len(df_input_1.columns))

scaler_1=MinMaxScaler()
data_scaled_1=scaler_1.fit_transform(df_input_1)
data_scaled_1

features_1=data_scaled_1[:,0:5]
target_1=data_scaled_1[:,4] 

print('shape of features:',features_1.shape)

x_train_validate_1, x_test_1, y_train_validate_1, y_test_1= train_test_split(features_1, target_1, test_size=0.10, random_state=1, shuffle=False)
x_train_1, x_validate_1, y_train_1, y_validate_1= train_test_split(x_train_validate_1, y_train_validate_1, test_size=0.25, random_state=1, shuffle=False)

win_length_1=1
batch_1=32
num_features_1=5
sampling_1= 1
#num_stride_new=1

train_generator_1=TimeseriesGenerator(x_train_1, y_train_1, length=win_length_1, sampling_rate=sampling_1, batch_size=batch_1)
validate_generator_1=TimeseriesGenerator(x_validate_1, y_validate_1, length=win_length_1, sampling_rate=sampling_1, batch_size=batch_1)
test_generator_1=TimeseriesGenerator(x_test_1, y_test_1, length=win_length_1, sampling_rate=sampling_1, batch_size=batch_1)

model_1=tf.keras.Sequential()
model_1.add(tf.keras.layers.LSTM(55, activation='relu',recurrent_activation='sigmoid',input_shape= (win_length_1, num_features_1),return_sequences=False))
#model_1.add(tf.keras.layers.Dropout(0.3))
#model_1.add(tf.keras.layers.LSTM(64,activation='relu',recurrent_activation='sigmoid',return_sequences=False))
#model_1.add(tf.keras.layers.Dropout(0.3))
model_1.add(tf.keras.layers.Dense(1))
model_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),loss=tf.keras.losses.MeanSquaredError(),metrics=[tf.keras.metrics.MeanSquaredError()])
model_1.summary()


#Callbacks
callbacks_1 = [tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=10,
    verbose=2,
    mode="min",
    restore_best_weights=True)]

#model fit
model_history_1=model_1.fit(
    train_generator_1, 
    epochs=50,
    verbose=1,
    callbacks=callbacks_1,
    validation_data=validate_generator_1,
    shuffle=False,)

print('********************************************************************************')

print('Training accuracy:', model_1.evaluate(train_generator_1, verbose=1))
print('Evaluation accuracy:',model_1.evaluate(validate_generator_1, verbose=1))
print('Testing accuracy:',model_1.evaluate(test_generator_1, verbose=1))

print('********************************************************************************')

#Training data predictions
model_1.predict(validate_generator_1, verbose=1)

#Testing Data preidctions
predictions_1=model_1.predict(test_generator_1, verbose=1)
predictions_1.shape[0]
#predictions

print('predictions shape:',predictions_1.shape)
print('y_test shape:',y_test_1.shape)
y_test_acc_1 = np.delete(y_test_1, slice(0,1), axis=0)
print('y_test_acc shape:', y_test_acc_1.shape)

print('----------------------------------------------')

print('Mean Absolute Error:', (metrics.mean_absolute_error(y_test_acc_1, predictions_1)).round(3))
print('Mean Squared Error:', (metrics.mean_squared_error(y_test_acc_1, predictions_1)).round(3))
print('Root Mean Squared Error:', (np.sqrt(metrics.mean_squared_error(y_test_acc_1, predictions_1))).round(3))

print('----------------------------------------------')

mape_1=metrics.mean_absolute_percentage_error(y_test_acc_2, predictions_2)
print('Mean Absolute Percentage Error:', (100*mape_1).round(1),'%')

print('----------------------------------------------')

print('Accuracy',(100-100*mape_1).round(1),'%')

print('----------------------------------------------')

print('R2 score for test set: ')
print((r2_score(y_test_acc_1,predictions_1)).round(1))

print('*******************************************************************************************************')


print('*******************************************************************************************************')


loss_1=model_history_1.history['loss']
mean_squared_error_1=model_history_1.history['mean_squared_error']
val_loss_1=model_history_1.history['val_loss']
val_mean_squared_error_1=model_history_1.history['val_mean_squared_error']

list_name_1= loss_1,mean_squared_error_1,val_loss_1,val_mean_squared_error_1

df_history_1 = pd.DataFrame (list_name_1).transpose()
df_history_1.columns = ['loss','mean_squared_error','val_loss','val_mean_squared_error']
df_history_1.index.name = 'epochs'
df_history_1.shape

print('-------------------------')
print('-------------------------')

mpl.rcParams['figure.figsize']=(10,5)
# mpl.rcParams['axes.grid']=False

def visualize_loss(history, title):
    val_loss_1 = model_history_1.history["loss"]
    val_loss_1 = history.history["val_loss"]
    epochs_1 = range(len(loss_1))
    plt.figure()
    plt.plot(epochs_1, loss_1, "b", label="Training loss")
    plt.plot(epochs_1, val_loss_1, "r", label="Validation loss")
    plt.title(title)
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.show()

visualize_loss(model_history_1, "Training and Validation Loss")

print('-------------------------')
print('-------------------------')


print('y_test shape:',y_test_1.shape)
print('x_test shape:', x_test_1.shape)
print('x_test with lengh shape:',x_test_1[:,:][win_length_1:].shape)

print('-------------------------')

print('y_test shape:',y_test_1.shape)
print('x_test shape:', x_test_1.shape)
print('x_test with lengh shape:',x_test_1[:,:][win_length_1:].shape)

print('-------------------------')

df_pred_1=pd.concat([pd.DataFrame(x_test_1[:,:4][win_length_1:]), pd.DataFrame(predictions_1)], axis=1) 
print(df_pred_1.shape)
df_pred_1

print('-------------------------')

rev_trans_1=scaler_1.inverse_transform(df_pred_1)
rev_trans_1

print('-------------------------')

rev_trans_dataframe_1=pd.DataFrame(rev_trans_1)
rev_trans_dataframe_1

print('-------------------------')

df_final_1=df_input_1[predictions_1.shape[0]*-1:]
print('count:')
print( df_final_1.count())
df_final_1

print('-------------------------')

print('lengh of columns:',len(df_input_1.columns))
print('shape of features:',features_1.shape)

print('-------------------------')

df_final_1['Predicted Values']=rev_trans_1[:,4]
df_final_1.head()

print('-------------------------')
print('-------------------------')

mpl.rcParams['figure.figsize']=(10,5)
# mpl.rcParams['axes.grid']=False

df_final_1[["Universal Display",'Predicted Values']].plot()

from keras.utils.vis_utils import plot_model
plot_model(model_1, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

"""**2) Baseline model**
* N:B: Please for each firm stock price:update the name of the firm,  model lookback, and nodes to avoid fitting and underfitting
"""

df_input_2=df[['S&P 500',	'VIX'	,'USD','Universal Display']]

print('lengh of columns:',len(df_input_2.columns))

scaler_2=MinMaxScaler()
data_scaled_2=scaler_2.fit_transform(df_input_2)
data_scaled_1

features_2=data_scaled_2[:,0:4]
target_2=data_scaled_2[:,3] 

print('shape of features:',features_2.shape)

x_train_validate_2, x_test_2, y_train_validate_2, y_test_2= train_test_split(features_2, target_2, test_size=0.10, random_state=12, shuffle=False)
x_train_2, x_validate_2, y_train_2, y_validate_2= train_test_split(x_train_validate_2, y_train_validate_2, test_size=0.25, random_state=12, shuffle=False)

win_length_2=1
batch_2=32
num_features_2=4
sampling_2= 1
#num_stride_new=1

train_generator_2=TimeseriesGenerator(x_train_2, y_train_2, length=win_length_2, sampling_rate=sampling_2, batch_size=batch_2)
validate_generator_2=TimeseriesGenerator(x_validate_2, y_validate_2, length=win_length_2, sampling_rate=sampling_2, batch_size=batch_2)
test_generator_2=TimeseriesGenerator(x_test_2, y_test_2, length=win_length_2, sampling_rate=sampling_2, batch_size=batch_2)

model_2=tf.keras.Sequential()
model_2.add(tf.keras.layers.LSTM(55, activation='relu',recurrent_activation='sigmoid',input_shape= (win_length_2, num_features_2),return_sequences=False))
#model_2.add(tf.keras.layers.Dropout(0.3))
#model_2.add(tf.keras.layers.LSTM(64,activation='relu',recurrent_activation='sigmoid',return_sequences=False))
#model_2.add(tf.keras.layers.Dropout(0.3))
model_2.add(tf.keras.layers.Dense(1))
model_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),loss=tf.keras.losses.MeanSquaredError(),metrics=[tf.keras.metrics.MeanSquaredError()])
model_2.summary()


#Callbacks
callbacks_2 = [tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=10,
    verbose=2,
    mode="min",
    restore_best_weights=True)]

#model fit
model_history_2=model_2.fit(
    train_generator_2, 
    epochs=50,
    verbose=1,
    callbacks=callbacks_2,
    validation_data=validate_generator_2,
    shuffle=False,)

print('********************************************************************************')

print('Training accuracy:', model_2.evaluate(train_generator_2, verbose=1))
print('Evaluation accuracy:',model_2.evaluate(validate_generator_2, verbose=1))
print('Testing accuracy:',model_2.evaluate(test_generator_2, verbose=1))

print('********************************************************************************')

#Training data predictions
model_2.predict(validate_generator_2, verbose=1)

#Testing Data preidctions
predictions_2=model_2.predict(test_generator_2, verbose=1)
predictions_2.shape[0]
#predictions

print('predictions shape:',predictions_2.shape)
print('y_test shape:',y_test_2.shape)
y_test_acc_2 = np.delete(y_test_2, slice(0,1), axis=0)
print('y_test_acc shape:', y_test_acc_2.shape)

print('----------------------------------------------')

print('Mean Absolute Error:', (metrics.mean_absolute_error(y_test_acc_2, predictions_2)).round(3))
print('Mean Squared Error:', (metrics.mean_squared_error(y_test_acc_2, predictions_2)).round(3))
print('Root Mean Squared Error:', (np.sqrt(metrics.mean_squared_error(y_test_acc_2, predictions_2))).round(3))

print('----------------------------------------------')

mape_2=metrics.mean_absolute_percentage_error(y_test_acc_2, predictions_2)
print('Mean Absolute Percentage Error:', (100*mape_2).round(1),'%')

print('----------------------------------------------')

print('Accuracy',(100-100*mape_2).round(1),'%')

print('----------------------------------------------')

print('R2 score for test set: ')
print((r2_score(y_test_acc_2,predictions_2)).round(1))

print('*******************************************************************************************************')


print('*******************************************************************************************************')


loss_1=model_history_2.history['loss']
mean_squared_error_2=model_history_2.history['mean_squared_error']
val_loss_2=model_history_2.history['val_loss']
val_mean_squared_error_2=model_history_2.history['val_mean_squared_error']

list_name_2= loss_2,mean_squared_error_2,val_loss_2,val_mean_squared_error_2

df_history_2 = pd.DataFrame (list_name_2).transpose()
df_history_2.columns = ['loss','mean_squared_error','val_loss','val_mean_squared_error']
df_history_2.index.name = 'epochs'
df_history_2.shape

print('-------------------------')
print('-------------------------')

mpl.rcParams['figure.figsize']=(10,5)
# mpl.rcParams['axes.grid']=False

def visualize_loss(history, title):
    val_loss_2 = model_history_2.history["loss"]
    val_loss_2 = history.history["val_loss"]
    epochs_2 = range(len(loss_2))
    plt.figure()
    plt.plot(epochs_2, loss_2, "b", label="Training loss")
    plt.plot(epochs_2, val_loss_2, "r", label="Validation loss")
    plt.title(title)
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.show()

visualize_loss(model_history_2, "Training and Validation Loss")

print('-------------------------')
print('-------------------------')


print('y_test shape:',y_test_2.shape)
print('x_test shape:', x_test_2.shape)
print('x_test with lengh shape:',x_test_2[:,:][win_length_2:].shape)

print('-------------------------')

print('y_test shape:',y_test_2.shape)
print('x_test shape:', x_test_2.shape)
print('x_test with lengh shape:',x_test_2[:,:][win_length_2:].shape)

print('-------------------------')

df_pred_2=pd.concat([pd.DataFrame(x_test_2[:,:4][win_length_2:]), pd.DataFrame(predictions_2)], axis=1) 
print(df_pred_2.shape)
df_pred_2

print('-------------------------')

rev_trans_2=scaler_2.inverse_transform(df_pred_2)
rev_trans_2

print('-------------------------')

rev_trans_dataframe_2=pd.DataFrame(rev_trans_2)
rev_trans_dataframe_2

print('-------------------------')

df_final_2=df_input_2[predictions_2.shape[0]*-1:]
print('count:')
print( df_final_2.count())
df_final_2

print('-------------------------')

print('lengh of columns:',len(df_input_2.columns))
print('shape of features:',features_2.shape)

print('-------------------------')

df_final_2['Predicted Values']=rev_trans_2[:,4]
df_final_2.head()

print('-------------------------')
print('-------------------------')

mpl.rcParams['figure.figsize']=(10,5)
# mpl.rcParams['axes.grid']=False

df_final_2[["Universal Display",'Predicted Values']].plot()

from keras.utils.vis_utils import plot_model
plot_model(model_2, to_file='model_plot.png', show_shapes=True, show_layer_names=True)